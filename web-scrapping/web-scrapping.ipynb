{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbyqXH/YpOiRWSST9KE01u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":71,"metadata":{"id":"RlcXqzKshPz1","executionInfo":{"status":"ok","timestamp":1686291332039,"user_tz":-330,"elapsed":401,"user":{"displayName":"NISHITA GOLE","userId":"11857374993239321034"}}},"outputs":[],"source":["import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","import numpy as np\n","from google.colab import files"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"SJibC1sphbz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#importing input file\n","df=pd.read_excel('/content/drive/MyDrive/Blackcoffer/Input.xlsx')[['URL_ID','URL']]"],"metadata":{"id":"6QoSy2m7hh_p","executionInfo":{"status":"ok","timestamp":1686291336660,"user_tz":-330,"elapsed":19,"user":{"displayName":"NISHITA GOLE","userId":"11857374993239321034"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["df=df.iloc[0:150]\n","df"],"metadata":{"id":"A_I5Jr8wiZqn","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1686291336660,"user_tz":-330,"elapsed":18,"user":{"displayName":"NISHITA GOLE","userId":"11857374993239321034"}},"outputId":"f385f12f-92f5-4537-db6e-99761ead7abf"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     URL_ID                                                URL\n","0        37  https://insights.blackcoffer.com/ai-in-healthc...\n","1        38  https://insights.blackcoffer.com/what-if-the-c...\n","2        39  https://insights.blackcoffer.com/what-jobs-wil...\n","3        40  https://insights.blackcoffer.com/will-machine-...\n","4        41  https://insights.blackcoffer.com/will-ai-repla...\n","..      ...                                                ...\n","109     146  https://insights.blackcoffer.com/blockchain-fo...\n","110     147  https://insights.blackcoffer.com/the-future-of...\n","111     148  https://insights.blackcoffer.com/big-data-anal...\n","112     149  https://insights.blackcoffer.com/business-anal...\n","113     150  https://insights.blackcoffer.com/challenges-an...\n","\n","[114 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-ed438799-8d68-4037-84ef-542c77186c9a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>URL_ID</th>\n","      <th>URL</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37</td>\n","      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38</td>\n","      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>39</td>\n","      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>40</td>\n","      <td>https://insights.blackcoffer.com/will-machine-...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41</td>\n","      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>109</th>\n","      <td>146</td>\n","      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n","    </tr>\n","    <tr>\n","      <th>110</th>\n","      <td>147</td>\n","      <td>https://insights.blackcoffer.com/the-future-of...</td>\n","    </tr>\n","    <tr>\n","      <th>111</th>\n","      <td>148</td>\n","      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n","    </tr>\n","    <tr>\n","      <th>112</th>\n","      <td>149</td>\n","      <td>https://insights.blackcoffer.com/business-anal...</td>\n","    </tr>\n","    <tr>\n","      <th>113</th>\n","      <td>150</td>\n","      <td>https://insights.blackcoffer.com/challenges-an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>114 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed438799-8d68-4037-84ef-542c77186c9a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ed438799-8d68-4037-84ef-542c77186c9a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ed438799-8d68-4037-84ef-542c77186c9a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["for index, row in df.iterrows():\n","  url = row['URL']\n","  url_id = row['URL_ID']\n","  try:\n","    response = requests.get(url,headers=header)\n","  except:\n","    print(\"can't get response of {}\".format(url_id))\n","\n","  #create a beautifulsoup object\n","  try:\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","  except:\n","    print(\"can't get page of {}\".format(url_id))\n","  #find title\n","  try:\n","    title = soup.find('h1').get_text()\n","  except:\n","    print(\"can't get title of {}\".format(url_id))\n","    continue\n","  #find text\n","  article = \"\"\n","  try:\n","    for p in soup.find_all('p'):\n","      article += p.get_text()\n","  except:\n","    print(\"can't get text of {}\".format(url_id))\n","\n","  #write title and text to the file\n","  file_name = '/content/drive/MyDrive/Blackcoffer/TitleText/' + str(url_id) + '.txt'\n","  with open(file_name, 'w') as file:\n","    file.write(title + '\\n' + article)"],"metadata":{"id":"nlnbHatU_5aT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_dir = \"/content/drive/MyDrive/Blackcoffer/TitleText\"\n","stopwords_dir = \"/content/drive/MyDrive/Blackcoffer/StopWords\"\n","sentiment_dir = \"/content/drive/MyDrive/Blackcoffer/MasterDictionary\""],"metadata":{"id":"bYY4EPhrKOni","executionInfo":{"status":"ok","timestamp":1686291338060,"user_tz":-330,"elapsed":38,"user":{"displayName":"NISHITA GOLE","userId":"11857374993239321034"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["import os\n","stopwords_set = set()\n","\n","# Iterate over the files in the directory\n","for filename in os.listdir(stopwords_dir):\n","    file_path = os.path.join(stopwords_dir, filename)\n","    try:\n","       with open(os.path.join(stopwords_dir,filename),'r',encoding='ISO-8859-1') as file:\n","            # Read the file and add its contents to the set of stop words\n","            stopwords_set.update(file.read().split())\n","    except IOError as e:\n","        print(f\"Error reading file: {file_path}\\n{str(e)}\")\n","\n","# Print the set of stop words\n","print(stopwords_set)\n"],"metadata":{"id":"H3Kh7pvwKc4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re"],"metadata":{"id":"ozHsAyiMMVB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = []\n","for text_file in os.listdir(text_dir):\n","  with open(os.path.join(text_dir,text_file),'r') as f:\n","    text = f.read()\n","    words = word_tokenize(text)\n","    filtered_text = [word for word in words if word.lower() not in stopwords_set]\n","    docs.append(filtered_text)"],"metadata":{"id":"EqsdyLPhLkQZ","executionInfo":{"status":"ok","timestamp":1686291338732,"user_tz":-330,"elapsed":677,"user":{"displayName":"NISHITA GOLE","userId":"11857374993239321034"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["docs"],"metadata":{"id":"KcgI4w2AModw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["positive_words = set()\n","negative_words = set()\n","\n","# Iterate over the files in the directory\n","for filename in os.listdir(sentiment_dir):\n","    file_path = os.path.join(sentiment_dir, filename)\n","    try:\n","        with open(file_path, 'r',encoding='ISO-8859-1') as file:\n","            # Read the file and add its contents to the appropriate word set\n","            word_list = file.read().split()\n","            if 'positive' in filename:\n","                positive_words.update(word_list)\n","            elif 'negative' in filename:\n","                negative_words.update(word_list)\n","    except IOError as e:\n","        print(f\"Error reading file: {file_path}\\n{str(e)}\")"],"metadata":{"id":"kM2xh_GTPN-H","executionInfo":{"status":"ok","timestamp":1686291341078,"user_tz":-330,"elapsed":686,"user":{"displayName":"NISHITA GOLE","userId":"11857374993239321034"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["print(\"Positive words:\")\n","for word in positive_words:\n","    print(word)"],"metadata":{"id":"wb9cSorfytdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = text.lower().split()\n","\n","# Calculate the positive score\n","positive_score = sum(1 for token in tokens if token in positive_words)\n","\n","# Print the positive score\n","print(\"Positive Score:\", positive_score)"],"metadata":{"id":"kTDUlyPsy0o5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = text.lower().split()\n","\n","# Calculate the negative score\n","negative_score = sum(1 for token in tokens if token in negative_words)\n","\n","# Print the negative score\n","print(\"Negative Score:\", negative_score)"],"metadata":{"id":"jPxWXPX42RK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n","Polarity_Score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n","print('polarity_score=', Polarity_Score)\n","     "],"metadata":{"id":"gackf5lXwZr3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["positive_words = []\n","Negative_words =[]\n","positive_score = []\n","negative_score = []\n","polarity_score = []\n","subjectivity_score = []\n","\n","#iterate through the list of docs\n","for i in range(len(docs)):\n","  positive_words.append([word for word in docs[i] if word.lower() in positive_words])\n","  Negative_words.append([word for word in docs[i] if word.lower() in negative_words])\n","  positive_score.append(len(positive_words[i]))\n","  negative_score.append(len(Negative_words[i]))\n","  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n","  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))"],"metadata":{"id":"4C5Tf7sc2hzq","executionInfo":{"status":"ok","timestamp":1686291341633,"user_tz":-330,"elapsed":11,"user":{"displayName":"NISHITA GOLE","userId":"11857374993239321034"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["import nltk.data\n","\n","avg_sentence_length = []\n","\n","\n","\n","# Tokenize the text into sentences\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","sentences = tokenizer.tokenize(text)\n","\n","# Calculate the average sentence length\n","total_sentence_length = sum(len(sentence.split()) for sentence in sentences)\n","average_sentence_length = total_sentence_length / len(sentences)\n","\n","# Print the average sentence length\n","print(\"Average Sentence Length:\", average_sentence_length)\n"],"metadata":{"id":"IQShjkgl5Aw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["complex_words = []\n","\n","# Define the complexity criterion\n","complex_word_length = 3 \n","\n","# Tokenize the text\n","tokens = text.lower().split()\n","\n","# Calculate the count of complex words\n","complex_word_count = sum(1 for token in tokens if len(token) > complex_word_length)\n","percentage_complex_words = (complex_word_count / len(words)) * 100\n","# Print the count of complex words\n","print(\"Complex Word Count:\", complex_word_count)\n","print(\"Percentage of Complex Words:\", percentage_complex_words)\n"],"metadata":{"id":"UizfaPIV-bPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n","Fog_Index = 0.4 * (average_sentence_length + percentage_complex_words)\n","print('fog index= ',Fog_Index )"],"metadata":{"id":"l4lzTS9uxhfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","tokens = text.split()\n","\n","# Calculate the word count\n","word_count = len(tokens)\n","\n","# Print the word count\n","print(\"Word Count:\", word_count)\n"],"metadata":{"id":"l6ITZRCQpCTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","# Tokenize the text into sentences\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","sentences = tokenizer.tokenize(text)\n","\n","# Calculate the total number of words and sentences\n","total_words = 0\n","total_sentences = len(sentences)\n","\n","# Iterate over sentences and count words\n","for sentence in sentences:\n","    words = nltk.word_tokenize(sentence)\n","    total_words += len(words)\n","\n","# Calculate the average number of words per sentence\n","average_words_per_sentence = total_words / total_sentences\n","\n","# Print the average number of words per sentence\n","print(\"Average Words Per Sentence:\", average_words_per_sentence)\n"],"metadata":{"id":"ZKQKV5nPyVHq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pyphen"],"metadata":{"id":"AZu5li0_pRIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyphen\n","\n","# Create an instance of the Pyphen hyphenation object\n","dic = pyphen.Pyphen(lang='en')\n","\n","# Tokenize the text\n","tokens = text.lower().split()\n","\n","# Calculate the syllable count per word\n","syllable_counts = []\n","for token in tokens:\n","    syllables = dic.inserted(token).count('-') + 1\n","    syllable_counts.append(syllables)\n","\n","# Print the syllable count per word\n","for i, token in enumerate(tokens):\n","    print(f\"Word: {token} | Syllable Count: {syllable_counts[i]}\")\n","# Count the number of syllables in the word\n","syllable_count = len(dic.inserted(word).split('-'))\n","\n","# Print the syllable count\n","print(\"Syllable Count:\", syllable_count)"],"metadata":{"id":"1bMEBaDPqPu3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# List of personal pronouns\n","personal_pronouns = ['I', 'me', 'my', 'mine', 'myself',\n","                     'you', 'your', 'yours', 'yourself',\n","                     'he', 'him', 'his', 'himself',\n","                     'she', 'her', 'hers', 'herself',\n","                     'it', 'its', 'itself',\n","                     'we', 'us', 'our', 'ours', 'ourselves',\n","                     'they', 'them', 'their', 'theirs', 'themselves']\n","\n","# Count occurrences of personal pronouns\n","pronoun_count = 0\n","for pronoun in personal_pronouns:\n","    pronoun_count += text.lower().count(pronoun)\n","\n","# Print the count of personal pronouns\n","print(\"Count of Personal Pronouns:\", pronoun_count)\n"],"metadata":{"id":"XUvpFuSyy12N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize the text\n","words = text.split()\n","\n","# Calculate the total word length\n","total_word_length = sum(len(word) for word in words)\n","\n","# Calculate the average word length\n","average_word_length = total_word_length / len(words)\n","\n","# Print the average word length\n","print(\"Average Word Length:\", average_word_length)\n"],"metadata":{"id":"8dRLuoCRqwLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data={'positive_score':positive_score,'negative_score':negative_score,'Polarity_Score':Polarity_Score,'subjectiivity_score':subjectivity_score,'avg_senetence_length':average_sentence_length,'Percentage_of_Complex_words':percentage_complex_words,'Fog_Index':Fog_Index,'avg_no_of_words_per_sentence':average_words_per_sentence,'complex_Word_Count':complex_word_count,'word_count':word_count,'syllable_count':syllable_count,'personal_pronouns':pronoun_count,'avg_word_length':average_word_length}"],"metadata":{"id":"l-wl9rQSzDRb","executionInfo":{"status":"ok","timestamp":1686291810927,"user_tz":-330,"elapsed":412,"user":{"displayName":"NISHITA GOLE","userId":"11857374993239321034"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["output=pd.DataFrame()\n","output=output.append(data,ignore_index=True)\n","#output.columns=['positive_score','negative_score','Polarity_Score','subjectiivity_score','avg_senetence_length','Percentage_of_Complex_words','Fog_Index','avg_no_of_words_per_sentence','complex_Word_Count','word_count','syllable_count','personal_pronouns','avg_word_length']\n","output"],"metadata":{"id":"vm_xbLUT0xG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('output.csv', 'a') as f:#creating text file \n","   output.to_csv(f, index=False, header=False)\n","files.download('output.csv')"],"metadata":{"id":"81zQsZA21FwA"},"execution_count":null,"outputs":[]}]}